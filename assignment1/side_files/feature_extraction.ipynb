{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.io.wavfile\n",
    "from scipy import signal\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#import cmath\n",
    "#import math\n",
    "\n",
    "import librosa\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "\n",
    "import pycochleagram\n",
    "import pycochleagram.utils\n",
    "import pycochleagram.erbfilter, pycochleagram.subband\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Segment continuous sounds into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44100, (89376,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate, data = scipy.io.wavfile.read('tester.wav')\n",
    "sample_rate, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://librosa.org/doc/latest/generated/librosa.display.waveshow.html : \n",
    "#import librosa.display\n",
    "#librosa.display.waveshow(data.astype('float'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c/anaconda3/lib/python3.7/site-packages/scipy/signal/spectral.py:1966: UserWarning: nperseg = 256 is greater than input length  = 2, using nperseg = 2\n",
      "  .format(nperseg, input_length))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "noverlap must be less than nperseg.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13136/2730398311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#resampled = signal.resample(data, num=50000, t=None, axis=0, window=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnumber_overlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msample_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTFT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnoverlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_overlap\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# all have the same size already!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# noverlap: the number of samples two neighbouring DFT have in common / reverse of hop size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/signal/spectral.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(x, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, boundary, padded, axis)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0mscaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spectrum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stft'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboundary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         padded=padded)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZxx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/signal/spectral.py\u001b[0m in \u001b[0;36m_spectral_helper\u001b[0;34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, mode, boundary, padded)\u001b[0m\n\u001b[1;32m   1756\u001b[0m         \u001b[0mnoverlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoverlap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnoverlap\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnperseg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1758\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'noverlap must be less than nperseg.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1759\u001b[0m     \u001b[0mnstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnperseg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnoverlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: noverlap must be less than nperseg."
     ]
    }
   ],
   "source": [
    "resampled = data\n",
    "#resampled = signal.resample(data, num=50000, t=None, axis=0, window=None)\n",
    "number_overlap = 100\n",
    "sample_freq, segment_times, STFT = signal.stft(resampled,noverlap=number_overlap) # all have the same size already!\n",
    "# noverlap: the number of samples two neighbouring DFT have in common / reverse of hop size\n",
    "\n",
    "hop_size = int(segment_times[1] - number_overlap)\n",
    "\n",
    "#STFT = mel_filterbank(sample_rate, hop_size, segment_times, STFT)\n",
    "# of course its the filtered STFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00208697, 0.00208697, 0.00208697, ..., 0.0577503 , 0.04310206,\n",
       "       0.04603171])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (data - data.mean())/data.std()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fourier transform (FT)\n",
    "The FT should already be included in the packages imported in the feature extraction section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Audio signals carry redundant and/or irrelevant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature computation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Constant Q transform (CQT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.65983156e+00+6.79972656e-02j,  9.15170383e-01+3.66273064e+00j,\n",
       "         3.66660956e+00-1.63080181e+00j, ...,\n",
       "         7.04456052e-01+1.09329462e+01j,  9.87816360e+00-3.52447419e+00j,\n",
       "        -6.16920553e+00-8.11721933e+00j],\n",
       "       [ 2.54945656e+00+5.39669399e-02j,  1.71161015e+00-2.11115332e+00j,\n",
       "        -9.60309115e-01-2.94217262e+00j, ...,\n",
       "        -2.02370624e+00+5.68719245e-01j, -3.28376275e-01+2.13315666e+00j,\n",
       "         1.82255872e+00+1.19061541e+00j],\n",
       "       [ 2.02691785e+00-6.37737855e-02j,  3.75891283e-01-2.28048411e+00j,\n",
       "        -2.48404047e+00-1.48695467e+00j, ...,\n",
       "        -1.79115028e+00+6.80501562e+00j,  3.33635497e+00+7.05861442e+00j,\n",
       "         7.68117462e+00+3.14362034e+00j],\n",
       "       ...,\n",
       "       [-6.53483192e-03+3.98480162e-05j, -1.24696220e-02+3.12765863e-03j,\n",
       "        -2.38174066e-01-3.02976111e-01j, ...,\n",
       "        -1.10051988e-01+7.63132561e-02j, -4.22451370e-02+1.34174212e-02j,\n",
       "         1.17106218e-01+5.69362132e-02j],\n",
       "       [-2.22692983e-03+1.10279701e-04j,  8.65251706e-03+3.15605226e-03j,\n",
       "        -3.66122336e-02-3.79082348e-02j, ...,\n",
       "        -1.22835557e-01+1.50001997e-02j,  4.78919750e-02+8.35019078e-02j,\n",
       "         7.01217795e-02+1.38924676e-01j],\n",
       "       [ 2.92232212e-04-2.67762982e-05j,  3.35980609e-03-1.69966634e-02j,\n",
       "         1.09477317e-02+1.22762541e-02j, ...,\n",
       "        -1.80671203e-01+1.37423577e-01j,  5.36569072e-02+1.11632712e-01j,\n",
       "        -5.01243895e-02+1.49849695e-02j]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://librosa.org/doc/latest/generated/librosa.cqt.html :\n",
    "cqt = librosa.cqt(data) # y: audio time series. Multi-channel is supported.\n",
    "# it produces complex numbered output\n",
    "cqt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Mel-frequency Cepstral Coefficient (MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-151.10595029,  -13.1204574 ,   66.97937561, ...,   59.36023239,\n",
       "          58.12763292,   39.38341316],\n",
       "       [  86.04548271,  165.86102251,  188.55969025, ...,  199.36629245,\n",
       "         189.33434061,  172.48097178],\n",
       "       [ -37.72146522,  -27.48396419,  -45.62408759, ...,  -58.14896861,\n",
       "         -64.18582034,  -48.39701926],\n",
       "       ...,\n",
       "       [  -7.33296495,  -11.4583387 ,  -10.21777164, ...,    4.63169811,\n",
       "          -1.48179792,   -1.40228823],\n",
       "       [   0.63738928,   -6.75138996,   -8.15532307, ...,   -7.21083155,\n",
       "          -7.65327216,   -4.22483511],\n",
       "       [   6.55520787,    3.60593943,   -5.84548003, ...,   -1.12405118,\n",
       "          -5.63415054,   -5.48980859]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html :\n",
    "mfcc = librosa.feature.mfcc(data) # y: audio time series. Multi-channel is supported.\n",
    "mfcc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Cochleogram\n",
    "https://pycochleagram.readthedocs.io/en/latest/\n",
    "According to the github page:\n",
    "This package contains four main modules:\n",
    "\n",
    "- pycochleagram.erbfilter: Functions for generating ERB-cosine filters. These functions are available in the original MATLAB implementation.\n",
    "\n",
    "- pycochleagram.subband: Functions for performing subband decomposition using filterbanks made with erbfilter. These functions are available in the MATLAB implementation.\n",
    "\n",
    "- pycochleagram.cochleagram: Convenience methods for quickly generating cochleagrams. Also, provides functions for cochleagram inversion (i.e., generating a signal waveform from a cochleagram). These methods are not readily available in the MATLAB implementation (you would have to compose functions from pycochleagram.erbfilter and pycochleagram.subband). This is intended to help you get started.\n",
    "\n",
    "- pycochleagram.utils: A collection of helpful methods for working with cochleagram generation, including some plotting and audio playback functions, as well as some fft-like methods that allow for easy switching between fftpack (numpy/scipy) and fftw. NOTE: when working with pyaudio and the audio playback functions in utils, the sound output can be very loud. Take caution when working with this method.\n",
    "\n",
    "explanations: https://pycochleagram.readthedocs.io/en/latest/pycochleagram.html#module-pycochleagram.utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 89376)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# care about filters:\n",
    "'''\n",
    "pycochleagram.erbfilter.freq2erb(freq_hz)\n",
    "pycochleagram.erbfilter.make_erb_cos_filters(signal_length, sr, n, low_lim, hi_lim, full_filter=False, strict=False)\n",
    "pycochleagram.erbfilter.make_full_filter_set(filts, signal_length=None)\n",
    "'''\n",
    "# recommended for the upcoming cochlea creation:\n",
    "filterbank, center_f, f = pycochleagram.erbfilter.make_erb_cos_filters_nx(signal_length=data.shape[0], sr=sample_rate, n=18, low_lim=1/10**6, hi_lim=sample_rate/2, sample_factor=1, padding_size=None, full_filter=True, strict=True)\n",
    "# return 9.265 * np.log(1 + freq_hz / (24.7 * 9.265))\n",
    "# n as number filters, normal: 20-40\n",
    "# hi_lim=sample_rate/2 as the Nyquist limit\n",
    "filterbank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 89376)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cochleagram = pycochleagram.subband.generate_subband_envelopes_fast(signal=data, filters=filterbank, padding_size=None, fft_mode='auto', debug_ret_all=False)\n",
    "cochleagram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store file as ARFF in ASCII format\n",
    "How to save and define an ASCII file: https://fileinfo.com/extension/ascii <br>\n",
    "How to create an ARFF file: https://www.geeksforgeeks.org/how-to-create-arff-file-in-weka-tool/ <br>\n",
    "ARFF files are also ASCII files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 0\n",
    "# create the arff file which can be then used by weka:\n",
    "with open('first_arff.arff', 'w') as f:\n",
    "    f.write(('@relation firstset\\n@attribute sample_id integer\\n'))\n",
    "    for i in range(cochleagram.flatten().shape[0]):\n",
    "        f.write((f'@attribute feat{i} real\\n'))\n",
    "    f.write(('@attribute label integer\\n\\n@data\\n'))\n",
    "    \n",
    "# write simple sample into file:\n",
    "with open('first_arff.arff', 'a') as f:\n",
    "    f.write(('1,'+','.join(cochleagram.flatten().astype(str).tolist()))+','+str(label)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop over all samples for Task 1 (weight classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfeat_ar_col_number = 522\\nmax_signal_len = 133124\\nmax_signal\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Plan: we need MFCC for each sample of same length, however, different duration length gives us different\n",
    "# column number in STFT which represents time bins. Therefore, fix STFT according to the longest sample\n",
    "# snippets:\n",
    "max_duration = 0\n",
    "max_signal = None\n",
    "\n",
    "for file_ind, file in enumerate(glob.glob('wavs/*/*/*.wav')):\n",
    "    y, sr = librosa.load(file)\n",
    "\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    \n",
    "    if duration > max_duration:\n",
    "        max_duration = duration\n",
    "        max_signal = y\n",
    "        \n",
    "# don't care about resampling as it makes the signal only smaller\n",
    "\n",
    "sample_freq, segment_times, STFT_largest = signal.stft(max_signal,noverlap=0) \n",
    "# noverlap: the number of samples two neighbouring DFT have in common / reverse of hop size\n",
    "# --> It follow by setting noverlap to 0 we get the longest STFT\n",
    "\n",
    "feat_ar_col_number = STFT_largest.shape[1] \n",
    "# segment_times intervals of 256\n",
    "'''\n",
    "feat_ar_col_number = 522\n",
    "max_signal_len = 133124\n",
    "max_signal\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(number_labels, label_name):\n",
    "    label_ar = np.zeros((number_labels,))\n",
    "    label_dict = {'light':1, 'medium':2, 'heavy':3, 'left':1, 'right':2}\n",
    "    label_ind = label_dict[label_name]\n",
    "    return label_ind\n",
    "    label_ar[label_ind] = 1\n",
    "    return label_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in which kind of features you want to extract: m->mfcc, q->cqt, c->cochleagram, several at once, e.g. cxqm\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# give info which features to extract:\n",
    "wished_feat = input('Type in which kind of features you want to extract: m->mfcc, q->cqt, c->cochleagram, several at once, e.g. cxq') \n",
    "arff_name = 'feat_arff/heavy_right_cqt.arff'\n",
    "##arff_name = 'mfcc_try_yesorno_upsampled_strict_E_sr.arff'\n",
    "arff_name = 'feat_inputs/mfcc_c.arff'\n",
    "feat_list = wished_feat.split('x')\n",
    "feat_list.sort()\n",
    "\n",
    "label_size_counter = np.zeros((2,))\n",
    "label_direction_counter = 0\n",
    "\n",
    "sample_rate = 44100\n",
    "hop_val = 800#330\n",
    "#sample_freq, segment_times, \n",
    "STFT_largest = librosa.stft(max_signal,hop_length=hop_val) \n",
    "feat_ar_col_number = STFT_largest.shape[1] \n",
    "\n",
    "\n",
    "for file_ind, file in enumerate(glob.glob('wavs/*/*/*.wav')):\n",
    "\n",
    "    # find out labels and define an label array:\n",
    "    file_name_parts = file.split('/')\n",
    "    label_ele_size = file_name_parts[1]\n",
    "    label_ele_direction = file_name_parts[2]\n",
    "    file_name = ''.join(file_name_parts[-1].split('.')[0])\n",
    "    \n",
    "    label_ar_size = one_hot_encode(3, label_ele_size)\n",
    "    label_ar_direction = one_hot_encode(2, label_ele_direction)\n",
    "    \n",
    "    # load the current wav file/sample:\n",
    "    sample_rate, data = scipy.io.wavfile.read(file)\n",
    "    # normalize it:\n",
    "    data = (data - data.mean())/data.std()\n",
    "\n",
    "    # extract features:\n",
    "    \n",
    "    for method_ind, method in enumerate(feat_list):\n",
    "        \n",
    "        if method=='c':\n",
    "            \n",
    "            diff_signal = max_signal_len - len(data)\n",
    "            if diff_signal > 0:\n",
    "                filler_signal = np.zeros((diff_signal,))\n",
    "                data = np.concatenate((data, filler_signal), axis=0)\n",
    "            #print('sig shape', signa)\n",
    "            \n",
    "            # recommended for the upcoming cochlea creation:\n",
    "            filterbank, center_f, f = pycochleagram.erbfilter.make_erb_cos_filters_nx(signal_length=data.shape[0], sr=sample_rate, n=18, low_lim=1/10**6, hi_lim=sample_rate/2, sample_factor=1, padding_size=None, full_filter=True, strict=True)\n",
    "            # return 9.265 * np.log(1 + freq_hz / (24.7 * 9.265))\n",
    "            # n as number filters, normal: 20-40\n",
    "            # hi_lim=sample_rate/2 as the Nyquist limit\n",
    "        \n",
    "            intermediate = pycochleagram.subband.generate_subband_envelopes_fast(signal=data, filters=filterbank, padding_size=None, fft_mode='auto', debug_ret_all=False)\n",
    "            intermediate = intermediate.reshape((1,-1))\n",
    "        elif method=='m':\n",
    "            # https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html :\n",
    "            intermediate = librosa.feature.mfcc(data,n_mfcc=20,sr=sample_rate,hop_length=hop_val) # y: audio time series. Multi-channel is supported.  \n",
    "            # default sr=22050, sample_rate from read out data: 44100\n",
    "\n",
    "        elif method=='q':\n",
    "            # https://librosa.org/doc/latest/generated/librosa.cqt.html :\n",
    "            intermediate = abs(librosa.cqt(data,n_bins=20)) # y: audio time series. Multi-channel is supported.\n",
    "            # it produces complex numbered output\n",
    "\n",
    "        # merge features into one array:\n",
    "        if not method=='c':\n",
    "            diff_col = feat_ar_col_number - intermediate.shape[1]\n",
    "            if diff_col > 0:\n",
    "                filler = np.zeros((intermediate.shape[0],diff_col))\n",
    "                intermediate = np.concatenate((intermediate,filler),axis=1)\n",
    "            if diff_col < 0:\n",
    "                intermediate = intermediate[:,:diff_col]\n",
    "\n",
    "        # normalize values as they are inputted into methods:\n",
    "        intermediate = (intermediate - intermediate.mean())/intermediate.std()\n",
    "\n",
    "        if method_ind == 0:\n",
    "            feat_ar = intermediate.flatten()\n",
    "        else:\n",
    "            feat_ar = np.concatenate((feat_ar, intermediate.flatten()), axis=1)\n",
    "\n",
    "\n",
    "    loop_number = len(feat_ar.flatten())\n",
    "\n",
    "    # create an empty arff file which is then filled with samples when not existing so far:\n",
    "    if not os.path.exists(arff_name):\n",
    "        \n",
    "        with open(arff_name, 'w') as f:\n",
    "            \n",
    "            f.write(('@relation firstset\\n'))\n",
    "            \n",
    "            for i in range(loop_number):\n",
    "                \n",
    "                f.write((f'@attribute feat{i} real\\n'))\n",
    "                \n",
    "            f.write(('@attribute label_size {1,2,3}\\n@attribute label_direction {1,2}\\n\\n@data'))\n",
    "            \n",
    "            #f.write(('@attribute label_size_light {yes,no}\\n@attribute label_size_medium {yes,no}\\n@attribute label_size_heavy {yes,no}\\n@attribute label_direction_left {yes,no}\\n@attribute label_direction_right {yes,no}\\n\\n@data'))\n",
    "\n",
    "    # fill the arff file \n",
    "    # write simple sample into file:  \n",
    "    #label_ar_size = label_ar_size.astype(int).astype(str)\n",
    "    #label_ar_size[np.where(label_ar_size=='1')] = 'yes'\n",
    "    #label_ar_size[np.where(label_ar_size=='0')] = 'no'\n",
    "\n",
    "    #label_ar_direction = label_ar_direction.astype(int).astype(str)\n",
    "    #label_ar_direction[np.where(label_ar_direction=='1')] = 'yes'\n",
    "   # label_ar_direction[np.where(label_ar_direction=='0')] = 'no'\n",
    "    \n",
    "    with open(arff_name, 'a') as g:\n",
    "        \n",
    "        #g.write((f'\\n0,'+','.join(feat_ar.astype(float).astype(str).tolist()))+','+','.join(label_ar_size.astype(int).astype(str).tolist())+','+','.join(label_ar_direction.astype(int).astype(str).tolist()))\n",
    "        #if label_ar_size==3 and label_ar_direction==2:  \n",
    "        g.write((f'\\n'+','.join(feat_ar.astype(float).astype(str).tolist()))+','+str(label_ar_size)+','+str(label_ar_direction))\n",
    "         \n",
    "        # upsample every second sample which comes from a minority group:\n",
    "        if label_ar_size==1 and label_ar_direction==2:\n",
    "\n",
    "            if True: #label_size_counter[0]%2==0: # True:\n",
    "                g.write((f'\\n'+','.join(feat_ar.astype(float).astype(str).tolist()))+','+str(label_ar_size)+','+str(label_ar_direction))\n",
    "\n",
    "            label_size_counter[0] += 1\n",
    "\n",
    "            label_direction_counter += 1\n",
    "                \n",
    "        elif label_ar_size==3 and label_ar_direction==2:\n",
    "              \n",
    "            if True: #label_size_counter[1]%2==0: # True\n",
    "                g.write((f'\\n'+','.join(feat_ar.astype(float).astype(str).tolist()))+','+str(label_ar_size)+','+str(label_ar_direction))\n",
    "\n",
    "            label_size_counter[1] += 1\n",
    "            \n",
    "            label_direction_counter += 1\n",
    "        \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From WEKA follows the class distribution:<br>\n",
    "light 557<br>\n",
    "medium 1466<br>\n",
    "heavy 618<br>\n",
    "<br>\n",
    "left 1707<br>\n",
    "right 934<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification\n",
    "https://stackoverflow.com/questions/19950889/predicting-a-numerical-value-using-knn-in-weka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 estimate boundaries, distributions, class-membership"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 classify new data based on estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
